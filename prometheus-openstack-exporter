#!/usr/bin/python

"""
OpenStack exporter for the prometheus monitoring system

Copyright (C) 2016 Canonical, Ltd.
Authors:
  Jacek Nykis <jacek.nykis@canonical.com>
  Laurent Sesques <laurent.sesques@canonical.com>

Copyright 2017-2018 IRISA
Authors:
  Olivier Sallou <olivier.sallou@irisa.fr>
Modifications:
  Use API instead of python clients
  Add rabbitmq checks
  Add cinder metrics
  Add manila metrics



This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License version 3,
as published by the Free Software Foundation.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranties of
MERCHANTABILITY, SATISFACTORY QUALITY, or FITNESS FOR A PARTICULAR PURPOSE.
See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
"""

import argparse
import yaml
from os import environ as env
from os import rename, path
import traceback
from threading import Thread
import pickle
import requests
from time import sleep, time
import json
import subprocess
import re
import sys
if (sys.version_info > (3, 0)):
    from http.server import BaseHTTPRequestHandler
    from http.server import HTTPServer
    from socketserver import ForkingMixIn
else:
    from BaseHTTPServer import BaseHTTPRequestHandler
    from BaseHTTPServer import HTTPServer
    from SocketServer import ForkingMixIn
from prometheus_client import CollectorRegistry, generate_latest, Gauge, CONTENT_TYPE_LATEST
from netaddr import iter_iprange

try:
    from urlparse import urlparse
except Exception:
    from urllib.parse import urlparse


class DataGatherer(Thread):
    def __init__(self):
        Thread.__init__(self)
        self.daemon = True
        self.duration = 0
        self.refresh_interval = config.get('cache_refresh_interval', 900)
        self.cache_file = config['cache_file']

    def __get_token(self, creds):
        auth={"auth":
                 {"scope":
                     {"project":
                         {
                         "name": creds['tenant_name'],
                         "domain": {"name": creds['domain_name']}
                         }
                     },
                  "identity": {
                      "password": {
                            "user": {
                                  "domain": {"name": creds['domain_name']},
                                  "password": creds['password'],
                                  "name": creds['username']
                            }
                      },
                      "methods": ["password"]
                      }
                 }
             }

        r = requests.post(creds['auth_url'] + '/auth/tokens', json=auth)
        token = r.headers['X-Subject-Token']
        return token


    def openstack_query(self, endpoint, url, token, data=None):
        headers = {'X-Auth-Token': token, 'Content-Type': 'application/json', 'Accept': 'application/json'}

        if data:
            r = requests.get(endpoint + url, headers=headers, json=data)
        else:
            r = requests.get(endpoint + url, headers=headers)
        res = []
        try:
            res = r.json()
        except Exception as e:
            print('Failed to decode result for %s  %s  %s' % (endpoint, url, str(e)))
            print(r.text)
        return res

    def check_endpoints(self, creds, token):
        services = {}
        res = self.openstack_query(creds['auth_url'], '/services',token)
        if res and res['services']:
            for endpoint in res['services']:
                services[endpoint['id']] = endpoint['name']
        else:
            raise Exception('Failed to contact identity for services')

        res = self.openstack_query(creds['auth_url'], '/endpoints',token)
        endpoints = []
        if res and res['endpoints']:
            for endpoint in res['endpoints']:
                # Get public interfaces only
                if endpoint['interface'] == 'public':
                    if endpoint['url'].startswith('http://'):
                        continue
                    o = urlparse(endpoint['url'])
                    data={'url': o.scheme+'://'+o.netloc, 'name': services[endpoint['service_id']]}
                    endpoints.append(data)
        result = {}
        headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}
        for endpoint in endpoints:
            try:
                r = requests.get(endpoint['url'], headers=headers)
                if r.status_code not in [200, 300, 401, 404]:
                    result[endpoint['name']] = 0
                else:
                    result[endpoint['name']] = 1
            except Exception as e:
                    print('Failed to contact endpoint %s: %s' % (endpoint['url'], str(e)))
                    result[endpoint['name']] = 0
        return result

    def run(self):
        prodstack = {}

        while True:
            start_time = time()
            creds = {
                "domain_name": env['OS_USER_DOMAIN_NAME'],
                "username": env['OS_USERNAME'],
                "password": env['OS_PASSWORD'],
                "tenant_name": env['OS_PROJECT_NAME'],
                "tenant_id": env['OS_PROJECT_ID'],
                "auth_url": env['OS_AUTH_URL'],
                "compute_url": env['OS_COMPUTE_URL'],
                "network_url": env['OS_NETWORK_URL'],
                'cinder_url': env['OS_CINDER_URL'],
                'share_url': env['OS_SHARE_URL']
            }
            try:
                token = self.__get_token(creds)
                prodstack['project_domain'] = {}
                prodstack['tenants'] = []
                prodstack['hypervisors'] = []
                prodstack['services'] = []
                prodstack['flavors'] = []
                prodstack['aggregates'] = []
                prodstack['networks'] = []
                prodstack['routers'] = []
                prodstack['subnets'] = []
                prodstack['ports'] = []
                prodstack['floatingips'] = []
                prodstack['instances'] = []
                prodstack['shares'] = []
                prodstack['endpoints'] = self.check_endpoints(creds, token)
                res = self.openstack_query(creds['auth_url'], '/projects',token)
                if res and res['projects']:
                    prodstack['tenants'] = res['projects']
                    for project in prodstack['tenants']:
                        prodstack['project_domain'][project['id']] = project['domain_id']
                res = self.openstack_query(creds['compute_url'] + env['OS_PROJECT_ID'], '/os-hypervisors/detail',token)
                if res and res['hypervisors']:
                    prodstack['hypervisors'] = res['hypervisors']
                res = self.openstack_query(creds['compute_url'] + env['OS_PROJECT_ID'], '/os-services',token)
                if res and res['services']:
                    prodstack['services'] = res['services']
                res = self.openstack_query(creds['compute_url'] + env['OS_PROJECT_ID'], '/flavors/detail',token)
                if res and res['flavors']:
                    prodstack['flavors'] = res['flavors']
                res = self.openstack_query(creds['compute_url'], '/os-aggregates',token)
                if res and res['aggregates']:
                    prodstack['aggregates'] = res['aggregates']
                res = self.openstack_query(creds['network_url'], '/v2.0/networks',token)
                if res and res['networks']:
                    prodstack['networks'] = res['networks']
                res = self.openstack_query(creds['network_url'], '/v2.0/routers',token)
                if res and 'routers' in res and res['routers']:
                    prodstack['routers'] = res['routers']
                res = self.openstack_query(creds['network_url'], '/v2.0/subnets',token)
                if res and res['subnets']:
                    prodstack['subnets'] = res['subnets']
                res = self.openstack_query(creds['network_url'], '/v2.0/ports',token)
                if res and res['ports']:
                    prodstack['ports'] = res['ports']
                res = self.openstack_query(creds['network_url'], '/v2.0/floatingips',token)
                if res and 'floatingips' in res and res['floatingips']:
                    prodstack['floatingips'] = res['hypervisors']



                for tenant in prodstack['tenants']:
                    try:
                        res = self.openstack_query(creds['share_url'] + creds['tenant_id'], '/shares/detail?all_tenants=1&project_id='+tenant['id'],token)
                        if res and res['shares']:
                            for share in res['shares']:
                                prodstack['shares'].append({
                                        'tenant_id': tenant['id'],
                                        'size': share['size'],
                                        'voltype': share['share_proto']
                                })
                    except Exception as e:
                        print('Could not get share info for project ' + tenant['name'] + ': ' + str(e))                    

                # Instance info is very heavy, disable until we merge this bit with pantomath
                prodstack['instances'] = []
                prodstack['cinder_volumes'] = []
                prodstack['cinder_capacity'] = []

                res = self.openstack_query(creds['compute_url'] + creds['tenant_id'], '/servers/detail?all_tenants=1',token)
                if res and res['servers']:
                    for instance in res['servers']:
                        if instance['tenant_id'] not in prodstack['project_domain']:
                            continue
                        new_instance = {
                                        'tenant_id': instance['tenant_id'],
                                        'domain_id': prodstack['project_domain'][instance['tenant_id']],
                                        'flavor': {'id': instance['flavor']['id']},
                                        'status': instance['status']
                                    }
                        # print("## "+str(new_instance))
                        prodstack['instances'].append(new_instance)


                for tenant in prodstack['tenants']:
                    try:
                        res = self.openstack_query(creds['cinder_url'] + creds['tenant_id'], '/volumes/detail?all_tenants=1&project_id='+tenant['id'],token)
                        if res and res['volumes']:
                            for vol in res['volumes']:
                                # print(str(vol))
                                prodstack['cinder_volumes'].append({
                                        'tenant_id': tenant['id'],
                                        'size': vol['size'],
                                        'voltype': vol['volume_type']
                                })
                        res = self.openstack_query(creds['cinder_url'] + creds['tenant_id'], '/scheduler-stats/get_pools?detail=true',token)
                        pools = []
                        if res and res['pools']:
                            for pool in res['pools']:
                                if pool['name'] not in pools:
                                    pools.append(pool['name'])
                                    prodstack['cinder_capacity'].append({
                                        'pool': pool['name'],
                                        'capacity': pool['capabilities']['total_capacity_gb'],
                                        'free': pool['capabilities']['free_capacity_gb']
                                    })
                    except Exception as e:
                        print('Could not get volume info for project ' + tenant['name'] + ': ' + str(e))

                rabbit_queues = subprocess.check_output('/usr/sbin/rabbitmqctl list_queues', shell=True)
                lines = rabbit_queues.split('\n')
                nb_message = 0
                for line in lines:
                    res = re.search('\s+(\d+)', line)
                    if res:
                        nb_message += int(res.group(1))
                prodstack['rabbitmq_messages'] = nb_message
            except:
                # Ignore failures, we will try again after refresh_interfal.
                # Most of them are termporary ie. connectivity problmes
                # To alert on stale cache use openstack_exporter_cache_age_seconds metric
                print(traceback.format_exc())
            else:
                with open(self.cache_file + '.new', "wb+") as f:
                    pickle.dump((prodstack, ), f, pickle.HIGHEST_PROTOCOL)
                rename(self.cache_file + '.new', self.cache_file)
            self.duration = time() - start_time
            print("Stats loaded, sleeping")
            sleep(self.refresh_interval)

    def get_stats(self):
        registry = CollectorRegistry()
        labels = ['cloud']
        age = Gauge('openstack_exporter_cache_age_seconds',
                    'Cache age in seconds. It can reset more frequently '
                    'than scraping interval so we use Gauge',
                    labels, registry=registry)
        l = [config['cloud']]
        age.labels(*l).set(time() - path.getmtime(self.cache_file))
        duration = Gauge('openstack_exporter_cache_refresh_duration_seconds',
                         'Cache refresh duration in seconds.',
                         labels, registry=registry)
        duration.labels(*l).set(self.duration)
        return generate_latest(registry)


class Neutron():
    def __init__(self):
        self.registry = CollectorRegistry()
        self.prodstack = {}
        with open(config['cache_file'], 'rb') as f:
            self.prodstack = pickle.load(f)[0]

        self.tenant_map = {t['id']: t['name'] for t in self.prodstack['tenants']}
        self.network_map = {n['id']: n['name'] for n in self.prodstack['networks']}
        self.subnet_map = {n['id']: {'name': n['name'], 'pool': n['allocation_pools']} for n in self.prodstack['subnets']}
        self.routers = self.prodstack['routers']
        self.ports = self.prodstack['ports']
        self.floating_ips = self.prodstack['floatingips']

    def _get_router_ip(self, uuid):
        owner = "network:router_gateway"
        for port in self.ports:
            if port["device_id"] == uuid and port["device_owner"] == owner:
                return port["fixed_ips"][0]["ip_address"]

    def get_floating_ips(self):
        ips = {}
        for ip in self.floating_ips:
            subnet = self.network_map[ip['floating_network_id']]
            try:
                tenant = self.tenant_map[ip['tenant_id']]
            except KeyError:
                tenant = 'Unknown tenant ({})'.format(ip['tenant_id'])
            key = (config['cloud'], subnet, tenant, 'floatingip', ip['status'])
            if key in ips:
                ips[key] += 1
            else:
                ips[key] = 1
        return ips

    def get_router_ips(self):
        ips = {}
        for r in self.routers:
            if self._get_router_ip(r['id']):
                tenant = self.tenant_map[r['tenant_id']]
                subnet = self.network_map[r['external_gateway_info']['network_id']]
                key = (config['cloud'], subnet, tenant, 'routerip', r['status'])
                if key in ips:
                    ips[key] += 1
                else:
                    ips[key] = 1
        return ips

    def gen_subnet_size(self):
        labels = ['cloud', 'network_name']
        net_size = Gauge('neutron_net_size',
                         'Neutron networks size',
                         labels, registry=self.registry)
        for n in self.prodstack['networks']:
            size = 0
            for subnet in n['subnets']:
                for pool in self.subnet_map[subnet]['pool']:
                    size += len(list(iter_iprange(pool['start'], pool['end'])))
            l = [config['cloud'], self.network_map[n['id']]]
            net_size.labels(*l).set(size)

    def gen_subnet_usage(self):
        labels = ['cloud', 'network_name', 'status']
        net_usage = Gauge('neutron_net_usage',
                         'Neutron networks usage',
                         labels, registry=self.registry)
        nets = {}
        for n in self.prodstack['ports']:
            network_name = self.network_map[n['network_id']]
            if network_name not in nets:
                nets[network_name] = {}
            if n['status'] not in nets[network_name]:
                nets[network_name][n['status']] = 0
            nets[network_name][n['status']] += 1
        for n in list(nets.keys()):
            for s in list(nets[n].keys()):
                l = [config['cloud'], n, s]
                net_usage.labels(*l).set(nets[n][s])


    def get_stats(self):
        labels = ['cloud', 'subnet_name', 'tenant', 'ip_type', 'ip_status']
        ips = self.get_floating_ips()
        ips.update(self.get_router_ips())
        metrics = Gauge('neutron_public_ip_usage',
                        'Neutron floating IP and router IP usage statistics',
                        labels, registry=self.registry)
        for k, v in ips.items():
            metrics.labels(*k).set(v)
        self.gen_subnet_size()
        self.gen_subnet_usage()
        return generate_latest(self.registry)


class Nova():
    def __init__(self):
        self.registry = CollectorRegistry()
        self.prodstack = {}
        with open(config['cache_file'], 'rb') as f:
            self.prodstack = pickle.load(f)[0]
        self.hypervisors = self.prodstack['hypervisors']
        self.tenant_map = {t['id']: t['name'] for t in self.prodstack['tenants']}
        self.flavor_map = {f['id']: {'ram': f['ram'], 'disk': f['disk'], 'vcpus': f['vcpus']}
                           for f in self.prodstack['flavors']}
        self.aggregate_map = {}
        self.services_map = {}
        for s in self.prodstack['services']:
            if s['binary'] == 'nova-compute':
                self.services_map[s['host']] = s['status']
        for agg in self.prodstack['aggregates']:
            self.aggregate_map.update({i: agg['name'] for i in agg['hosts']})

    def _get_schedulable_instances(self, host):
            free_vcpus = host['vcpus'] * config['openstack_allocation_ratio_vcpu'] - host['vcpus_used']
            free_ram_mbs = host['memory_mb'] * config['openstack_allocation_ratio_ram'] - host['memory_mb_used']
            free_disk_gbs = host['local_gb'] * config['openstack_allocation_ratio_disk'] - host['local_gb_used']
            s = config['schedulable_instance_size']
            return min(int(free_vcpus / s['vcpu']),
                       int(free_ram_mbs / s['ram_mbs']),
                       int(free_disk_gbs / s['disk_gbs']))

    def gen_hypervisor_stats(self):
        labels = ['cloud', 'hypervisor_hostname', 'aggregate', 'nova_service_status']
        vms = Gauge('hypervisor_running_vms', 'Number of running VMs', labels, registry=self.registry)
        vcpus_total = Gauge('hypervisor_vcpus_total', 'Total number of vCPUs', labels, registry=self.registry)
        vcpus_used = Gauge('hypervisor_vcpus_used', 'Number of used vCPUs', labels, registry=self.registry)
        mem_total = Gauge('hypervisor_memory_mbs_total', 'Total amount of memory in MBs', labels, registry=self.registry)
        mem_used = Gauge('hypervisor_memory_mbs_used', 'Used memory in MBs', labels, registry=self.registry)
        disk_total = Gauge('hypervisor_disk_gbs_total', 'Total amount of disk space in GBs', labels, registry=self.registry)
        disk_used = Gauge('hypervisor_disk_gbs_used', 'Used disk space in GBs', labels, registry=self.registry)
        schedulable_instances = Gauge('hypervisor_schedulable_instances',
                                      'Number of schedulable instances, see "schedulable_instance_size" option',
                                      labels, registry=self.registry)

        for h in self.hypervisors:
            host = h['service']['host']
            l = [config['cloud'], host, self.aggregate_map.get(host, 'unknown'), self.services_map[host]]
            vms.labels(*l).set(h['running_vms'])
            vcpus_total.labels(*l).set(h['vcpus'])
            vcpus_used.labels(*l).set(h['vcpus_used'])
            mem_total.labels(*l).set(h['memory_mb'])
            mem_used.labels(*l).set(h['memory_mb_used'])
            disk_total.labels(*l).set(h['local_gb'])
            disk_used.labels(*l).set(h['local_gb_used'])
            if config.get("schedulable_instance_size", False):
                schedulable_instances.labels(*l).set(self._get_schedulable_instances(h))

    def gen_instance_stats(self):
        instances = Gauge('nova_instances',
                          'Nova instances metrics',
                          ['cloud', 'tenant', 'instance_state', 'domain'], registry=self.registry)
        res_ram = Gauge('nova_resources_ram_mbs',
                        'Nova RAM usage metric',
                        ['cloud', 'tenant'], registry=self.registry)
        res_vcpus = Gauge('nova_resources_vcpus',
                          'Nova vCPU usage metric',
                          ['cloud', 'tenant'], registry=self.registry)
        res_disk = Gauge('nova_resources_disk_gbs',
                         'Nova disk usage metric',
                         ['cloud', 'tenant'], registry=self.registry)
        for i in self.prodstack['instances']:
            if i['tenant_id'] in self.tenant_map:
                tenant = self.tenant_map[i['tenant_id']]
            else:
                tenant = 'orphaned'
            flavor = None
            if i['flavor']['id'] in self.flavor_map:
                flavor = self.flavor_map[i['flavor']['id']]

            instances.labels(config['cloud'], tenant, i['status'], i['domain_id']).inc()
            if flavor:
                res_ram.labels(config['cloud'], tenant).inc(flavor['ram'])
                res_vcpus.labels(config['cloud'], tenant).inc(flavor['vcpus'])
                res_disk.labels(config['cloud'], tenant).inc(flavor['disk'])

    def gen_overcommit_stats(self):
        labels = ['cloud', 'resource']
        openstack_overcommit = Gauge('openstack_allocation_ratio', 'Openstack overcommit ratios',
                                     labels, registry=self.registry)
        l = [config['cloud'], 'vcpu']
        openstack_overcommit.labels(*l).set(config['openstack_allocation_ratio_vcpu'])
        l = [config['cloud'], 'ram']
        openstack_overcommit.labels(*l).set(config['openstack_allocation_ratio_ram'])
        l = [config['cloud'], 'disk']
        openstack_overcommit.labels(*l).set(config['openstack_allocation_ratio_disk'])

    def gen_rabbitmq_stats(self):
        queues = Gauge('rabbitmq_queues',
                          'Rabbitmq pending messages metrics',
                          ['cloud'], registry=self.registry)
        queues.labels(config['cloud']).inc(self.prodstack['rabbitmq_messages'])

    def gen_cinder_stats(self):
        labels = ['cloud', 'tenant', 'voltype']
        cinder_volumes = Gauge('cinder_volumes',
                          'Cinder volumes metrics',
                          labels, registry=self.registry)
        for i in self.prodstack['cinder_volumes']:
            tenant = self.tenant_map[i['tenant_id']]
            cinder_volumes.labels(config['cloud'], tenant, i['voltype']).inc(i['size'])
        if self.prodstack.get('cinder_capacity', None):
            cinder_capacity = Gauge('cinder_capacity',
                            'Cinder pools total capacity',
                            ['cloud', 'pool'], registry=self.registry)          
            for i in self.prodstack['cinder_capacity']:
                cinder_capacity.labels(config['cloud'], i['pool']).set(i['capacity'])
            cinder_free = Gauge('cinder_free',
                            'Cinder pools free capacity',
                            ['cloud', 'pool'], registry=self.registry)          
            for i in self.prodstack['cinder_capacity']:
                cinder_free.labels(config['cloud'], i['pool']).set(i['free'])

    def gen_share_stats(self):
        labels = ['cloud', 'tenant', 'voltype']
        share_volumes = Gauge('share_volumes',
                          'Manila volumes metrics',
                          labels, registry=self.registry)
        for i in self.prodstack['shares']:
            tenant = self.tenant_map[i['tenant_id']]
            share_volumes.labels(config['cloud'], tenant, i['voltype']).inc(i['size'])

    def gen_endpoints_stats(self):
        labels = ['cloud', 'endpoint']
        endpoints = Gauge('openstack_endpoints_errors',
                          'API endpoints errors metrics',
                          labels, registry=self.registry)
        if 'endpoints' in self.prodstack:
            for i in list(self.prodstack['endpoints'].keys()):
                if self.prodstack['endpoints'][i] == 0:
                    endpoints.labels(config['cloud'], i).set(1)
                else:
                    endpoints.labels(config['cloud'], i).set(0)

    def get_stats(self):
        self.gen_hypervisor_stats()
        self.gen_instance_stats()
        self.gen_overcommit_stats()
        self.gen_rabbitmq_stats()
        self.gen_cinder_stats()
        self.gen_share_stats()
        self.gen_endpoints_stats()
        return generate_latest(self.registry)


class Swift():
    def __init__(self):
        self.registry = CollectorRegistry()
        self.recon_port = config.get('swift_recon_port', 6200)
        self.baseurl = 'http://{}:' + str(self.recon_port) + '/recon/{}'
        self.swift_hosts = config.get('swift_hosts', [])

    def gen_disk_usage_stats(self):
        labels = ['cloud', 'hostname', 'device', 'type']
        swift_disk = Gauge('swift_disk_usage_bytes', 'Swift disk usage in bytes',
                           labels, registry=self.registry)
        for h in self.swift_hosts:
            r = requests.get(self.baseurl.format(h, 'diskusage'))
            for disk in r.json():
                if not all([disk.get(i, False) for i in ['size', 'used', 'device']]):
                    continue
                swift_disk.labels(config['cloud'], h, disk['device'], 'size').set(int(disk['size']))
                swift_disk.labels(config['cloud'], h, disk['device'], 'used').set(int(disk['used']))

    def gen_quarantine_stats(self):
        labels = ['cloud', 'hostname', 'ring']
        swift_quarantine = Gauge('swift_quarantined_objects', 'Number of quarantined objects',
                                 labels, registry=self.registry)
        for h in self.swift_hosts:
            r = requests.get(self.baseurl.format(h, 'quarantined'))
            for ring in ['accounts', 'objects', 'containers']:
                swift_quarantine.labels(config['cloud'], h, ring).set(r.json().get(ring))

    def gen_replication_stats(self):
        labels = ['cloud', 'hostname', 'ring', 'type']
        swift_repl = Gauge('swift_replication_stats', 'Swift replication stats', labels, registry=self.registry)
        labels = ['cloud', 'hostname', 'ring']
        swift_repl_duration = Gauge('swift_replication_duration_seconds', 'Swift replication duration in seconds',
                                    labels, registry=self.registry)
        for h in self.swift_hosts:
            metrics = ['attempted', 'diff', 'diff_capped', 'empty',
                       'failure', 'hashmatch', 'no_change', 'remote_merge',
                       'remove', 'rsync', 'success', 'ts_repl']
            # Object replication is special
            r = requests.get(self.baseurl.format(h, 'replication/object'))
            swift_repl_duration.labels(config['cloud'], h, 'object').set(r.json()['object_replication_time'])
            for ring in ['account', 'container']:
                r = requests.get(self.baseurl.format(h, 'replication/' + ring))
                swift_repl_duration.labels(config['cloud'], h, ring).set(r.json()['replication_time'])
                for metric in metrics:
                    swift_repl.labels(config['cloud'], h, ring, metric).set(r.json()['replication_stats'][metric])

    def get_stats(self):
        self.gen_disk_usage_stats()
        self.gen_quarantine_stats()
        self.gen_replication_stats()
        return generate_latest(self.registry)


class ForkingHTTPServer(ForkingMixIn, HTTPServer):
    pass


class OpenstackExporterHandler(BaseHTTPRequestHandler):
    def __init__(self, *args, **kwargs):
        BaseHTTPRequestHandler.__init__(self, *args, **kwargs)

    def do_GET(self):
        url = urlparse(self.path)
        if url.path == '/metrics':
            try:
                neutron = Neutron()
                nova = Nova()
                swift = Swift()
                output = neutron.get_stats() + \
                    nova.get_stats() + \
                    swift.get_stats() + \
                    data_gatherer.get_stats()
                self.send_response(200)
                self.send_header('Content-Type', CONTENT_TYPE_LATEST)
                self.end_headers()
                if (sys.version_info > (3, 0)):
                    self.wfile.write(output.encode())
                else:
                    self.wfile.write(output)
            except:
                self.send_response(500)
                self.end_headers()
                if (sys.version_info > (3, 0)):
                    self.wfile.write(traceback.format_exc().encode())
                else:
                    self.wfile.write(traceback.format_exc())
        elif url.path == '/':
            self.send_response(200)
            self.end_headers()
            if (sys.version_info > (3, 0)):
                self.wfile.write(b"""<html>
                <head><title>OpenStack Exporter</title></head>
                <body>
                <h1>OpenStack Exporter</h1>
                <p>Visit <code>/metrics</code> to use.</p>
                </body>
                </html>""")
            else:
                self.wfile.write("""<html>
                <head><title>OpenStack Exporter</title></head>
                <body>
                <h1>OpenStack Exporter</h1>
                <p>Visit <code>/metrics</code> to use.</p>
                </body>
                </html>""")
        else:
            self.send_response(404)
            self.end_headers()


def handler(*args, **kwargs):
    OpenstackExporterHandler(*args, **kwargs)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(usage=__doc__,
                                     description='Prometheus OpenStack exporter',
                                     formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument('config_file', nargs='?',
                        help='Configuration file path',
                        default='/etc/prometheus/prometheus-openstack-exporter.yaml',
                        type=argparse.FileType('r'))
    args = parser.parse_args()
    config = yaml.safe_load(args.config_file.read())
    data_gatherer = DataGatherer()
    data_gatherer.start()
    server = ForkingHTTPServer(('', config.get('listen_port')), handler)
    server.serve_forever()
